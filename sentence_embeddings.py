# -*- coding: utf-8 -*-
"""CV Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QJdGnX6GctRC2oJNrvW2hAnCpTrglagb
"""

# !pip install pytorch-transformers

# !pip install transformers

# from google.colab import files
# uploaded = files.upload()
import argparse
import os
import pickle
from random import randint
parser = argparse.ArgumentParser(description='algo to use for embeddings')
parser.add_argument('--algo', type=str, default='doc2vec', metavar='a',
                    help="algo to create embeddings: bert,doc2vec, infersent, VSE")

args = parser.parse_args()

import pandas as pd
table_data = pd.read_csv('/scratch/ans698/wiki_movie_plots_deduped.csv')['Release Year']
# print(dataset.head())

# print(len(dataset['Plot'][0]))
# max_len = dataset['Plot'].map(len).idxmax()
# print(max_len)
# dataset.iloc[26064]

# filtered_dataset = dataset[dataset['Plot'].str.len() <= 512]
# print(len(filtered_dataset))
# print(filtered_dataset['Plot'][0])

directory = '/scratch/ans698/combined_dataset/plot'
dataset = []
year = []

if os.path.isfile('plots.p'):
    dataset = pickle.load(open( "plots.p", "rb" ))
    year = pickle.load(open( "year.p", "rb" ))
else:
	for filename in sorted(os.listdir(directory)):
		with open(directory+'/'+filename, 'rb') as f:
			dataset.append(f.read().decode(errors='ignore'))
			year.append(table_data[int(filename.split('.')[0])])
	pickle.dump(dataset,open( "plots.p", "wb" ))
	pickle.dump(year,open( "year.p", "wb" ))

print(dataset[0])
print(year[0])
print(len(dataset))




def DistilBert():
	import torch
	from transformers import DistilBertTokenizer,DistilBertModel
	# Transformers has a unified API
	# for 8 transformer architectures and 30 pretrained weights.
	#          Model          | Tokenizer          | Pretrained weights shortcut
	# MODELS = [(BertModel,       BertTokenizer,       'bert-base-uncased'),
	#           (OpenAIGPTModel,  OpenAIGPTTokenizer,  'openai-gpt'),
	#           (GPT2Model,       GPT2Tokenizer,       'gpt2'),
	#           (CTRLModel,       CTRLTokenizer,       'ctrl'),
	#           (TransfoXLModel,  TransfoXLTokenizer,  'transfo-xl-wt103'),
	#           (XLNetModel,      XLNetTokenizer,      'xlnet-base-cased'),
	#           (XLMModel,        XLMTokenizer,        'xlm-mlm-enfr-1024'),
	#           (DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased'),
	#           (RobertaModel,    RobertaTokenizer,    'roberta-base')]

	tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
	encoded_text = dataset.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))
	# model.resize_token_embeddings(len(encoded_text))
	print(encoded_text[0])
	print(len(encoded_text))
	print(dataset.map(len).max())
	model = DistilBertModel.from_pretrained('distilbert-base-uncased')

	import numpy as np
	max_len = 512
	# for i in encoded_text.values:
	#     if len(i) > max_len:
	#         max_len = len(i)

	padded = np.array([i + [0]*(max_len-len(i)) for i in encoded_text.values])

	attention_mask = np.where(padded != 0, 1, 0)
	print(attention_mask.shape)

	input_ids = torch.tensor(padded)  
	attention_mask = torch.tensor(attention_mask)

	with torch.no_grad():
	    last_hidden_states = model(input_ids, attention_mask=attention_mask)

	embeddings = last_hidden_states[0][:,0,:].numpy()
	print(embeddings[0])
	print(embeddings.shape)
	return embeddings

def doc2vec():
	from gensim.models.doc2vec import Doc2Vec, TaggedDocument
	import nltk
	from tqdm import tqdm
	import multiprocessing

	def tokenize_text(text):
	    tokens = []
	    for sent in tqdm(nltk.sent_tokenize(text)):
	        for word in nltk.word_tokenize(sent):
	            if len(word) < 2:
	                continue
	            tokens.append(word.lower())
	    return tokens
	tagged_data = [TaggedDocument(words=tokenize_text(_d.lower()), tags=[year[i]]) for i, _d in enumerate(dataset)]
	cores = multiprocessing.cpu_count()
	print(tagged_data[0])
	max_epochs = 30
	vec_size = 512
	alpha = 0.025

	model = Doc2Vec(vector_size=vec_size,
	                alpha=alpha, 
	                min_alpha=0.000025,
	                min_count=2,
	                workers=cores,
	                negative=5,
	                hs=0,
	                dm =1)
	  
	model.build_vocab([x for x in tqdm(tagged_data)])


	# for epoch in range(max_epochs):
	#     print('iteration {0}'.format(epoch))
	#     model.train(tagged_data,
	#                 total_examples=model.corpus_count,
	#                 epochs=model.iter)
	#     # decrease the learning rate
	#     model.alpha -= 0.0002
	#     # fix the learning rate, no decay
	#     model.min_alpha = model.alpha
	model.train(tagged_data,total_examples=model.corpus_count, epochs=max_epochs)


	model.save("d2v.model")
	print("Model Saved")

	return [model.infer_vector(doc.words, steps=20) for doc in tagged_data]


def inferSent():
	import nltk
	# nltk.download('punkt')
	from InferSent.models import InferSent
	import torch

	# use_cuda = True
	device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
	# model = model.cuda() if use_cuda else model

	# V = 2
	MODEL_PATH = 'encoder/infersent2.pkl'
	params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,
	                'pool_type': 'max', 'dpout_model': 0, 'version': 2}
	infersent = InferSent(params_model).to(device)
	infersent.load_state_dict(torch.load(MODEL_PATH))
	W2V_PATH = 'fastText/crawl-300d-2M.vec'
	infersent.set_w2v_path(W2V_PATH)
	print('set w2v')

	infersent.build_vocab(dataset, tokenize=True)
	embeddings = infersent.encode(dataset,bsize=64, tokenize=True)
	idx = randint(0, len(dataset))
	_, _ = infersent.visualize(dataset[idx])
	print('done')
	return embeddings




def VSE():
	embeddings = []
	return embeddings

if args.algo == 'doc2vec':
	embeddings = doc2vec()
	pickle.dump( embeddings, open( "doc2vecEmbeddings.p", "wb" ) )

elif args.algo == 'bert':
	embeddings = DistilBert()
	pickle.dump( embeddings, open( "bertEmbeddings.p", "wb" ) )
elif args.algo == 'infersent':
	embeddings = inferSent()
	pickle.dump( embeddings, open( "infersentEmbeddingsGPU300.p", "wb" ) )
	print(type(embeddings))
	print(embeddings[0])
	print(embeddings.shape)
	print(embeddings[0].shape)
	
elif args.algo == 'vse':
	VSE()
else:
	print('WRONG')